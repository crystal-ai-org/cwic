[DEFAULT]
test-name = hello

[experiment]
dataset = Compilation
basemodel = Llama-3.2-1B

distill_temperature = 1.0
flop_loss_weight = 1.0
# IMPORTANT: due to the way the flop penalty works over a whole gradient batch:
# increasing grad_accums is NOT the same as a larger fulll gradient batch size with grad_accums=1, (use more gpus)
# the larger full gradient batch size with perform BETTER
# ye been warned!
# our runs were trained with 128 batch size across devices but doing more would be awesome
grad_accums = 1

max_train_seq_len = 1024
epochs = 3
dataset_length = 1674108

flop_ratio_max = 6
flop_schedule_length = 40000

optimizer = Adam
beta1 = 0.9
beta2 = 0.95
weight_decay = 0.01

[logging]
hist_interval = 100
test_interval = 100
wandb_project = cwic

[ablations]
flop_warmup = True
stripe_size = 1024
stripe_size_lm_head = 1024
max_flop_ratio_LMHead = False