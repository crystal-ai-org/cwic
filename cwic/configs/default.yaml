
run_name: torch-training-iso

wandb_entity: null
wandb_project: cwic

teacher_model: unsloth/Llama-3.2-1B-Instruct

batch_size: 8
gradient_accumulations: 16

checkpoint_interval: 1000

start_compute_reduction: 1.0
end_compute_reduction: 6.0
compute_reduction_steps: 10000
model:

  stripe_size: 1024
  head_stripe_size: 8192

  head_limit: 100.0

  threshold_lr_scale: 10.0
  threshold_init: 0.0
  threshold_minimum: 0.001

  bandwidth: 0.1

  stats_beta: 0.99
  median_iters: 3


dataset:

  path: crystal-ai/chat-compilation-benchmark-5x-Llama-3.2-Instruct-Shuffled

  split: train
  streaming: true


optimizer:

  lr: 0.00004898979485566357
  weight_decay: 0.01

  betas: [0.9, 0.95]


lr_scheduler:

  name: linear

  num_warmup_steps: 400
  num_training_steps: 10000
